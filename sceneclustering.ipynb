{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"K-histograms\" for catergorizing keyframes from a camera feed by unique scene\n",
    "-----\n",
    "\n",
    "Problem:  Categorizing frames from surveilance footage by unique \"scene\" (tilt, pan, zoom etc.).\n",
    "\n",
    "Solution:  Clustering by comparing histograms of greyscale pixels from fixed-sized patches in a fixed-sized region-of-interest (ROI) from the camera, across all frames.\n",
    "\n",
    "Example:  You have a surveilance camera that alternates between three different scenes on a fixed schedule and you do not know 'ex ante' to which scene a frame belongs.\n",
    "\n",
    "Routine:\n",
    "1. User selects size of fixed tiles according to imagery features.\n",
    "2. User selects a ROI in the scene.\n",
    "3. User selects a threshold to compare bilevel histograms.\n",
    "4. User defines number of unique scenes from empirical evidence.\n",
    "5. Program will crop scene to ROI and desaturate image according to the ITU-R 601-2 luma transform.\n",
    "6. Program will \"equalize histogram\" of greyscale intensity values across entire ROI.\n",
    "7. Program will calculate bilevel histograms for all tiles in the ROI for the frame according to user-defined threshold.\n",
    "8. Program will flatten all histograms in a frame sequentially, ensuring that spatial information about the image is preserved during clustering.\n",
    "9. Program will attempt to cluster the frames into the number of scenes defined by the user, using the Euclidean distance metric to evaluate similarity of the flattened spatial histograms across all frames.\n",
    "10. Program will return labels of all the frames.\n",
    "\n",
    "Analysis:  Allows the user to view plots of clustering behavior, as well as to identify the frames that are most-difficult to categorize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Settings for Frames, Scenes, Tiles and Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "# LOCATION OF FRAMES TO CLUSTER\n",
    "frame_dir = \"frames\"\n",
    "files = [os.path.join(frame_dir,d) for d in sorted(os.listdir(frame_dir)) if not d.startswith('.')]  \n",
    "num_frames = len(files)\n",
    "\n",
    "# FRAME and SCENE INFORMATION\n",
    "frame_height = 720\n",
    "frame_width = 1280\n",
    "pixel_depth = 255  # Number of levels per pixel.\n",
    "num_scenes = 3\n",
    "\n",
    "# REGION OF INTEREST (ROI) INFORMATION\n",
    "tile_width = 80  # desired width of a tile.\n",
    "tile_height = 80  # desired height of a tile.\n",
    "\n",
    "tiles_per_row = int(frame_width/tile_width) # 16\n",
    "tiles_per_column = int(frame_height/tile_height) # 9\n",
    "tiles_per_frame = tiles_per_row * tiles_per_column # 144\n",
    "\n",
    "# INDEXING FROM ZERO IN THE TOP LEFT CORNER, \n",
    "# FROM LEFT TO RIGHT, AND TOP TO BOTTOM,\n",
    "# INDICATE A LIST OF TILES THAT CONSTITUE THE ROI:\n",
    "roi_tiles = range(32,80) # corresponds to 3rd, 4th, and 5th rows\n",
    "num_roi_tiles = len(roi_tiles) # 48\n",
    "\n",
    "# HISTOGRAM SETTINGS\n",
    "num_bins = 2 # 2 for bilevel comparison\n",
    "threshold = 15 # roughly corresponds to isloating the darkest 10% of pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proprietary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_histogram_equalization(image, number_bins=256):\n",
    "  # from http://www.janeriksolem.net/2009/06/histogram-equalization-with-python-and.html\n",
    "  # https://en.wikipedia.org/wiki/Histogram_equalization\n",
    "  # image is a numpy array\n",
    "    \n",
    "  # get image histogram\n",
    "  image_histogram, bins = np.histogram(image.flatten(), number_bins, normed=True)\n",
    "  cdf = image_histogram.cumsum() # cumulative distribution function\n",
    "  cdf = 255 * cdf / cdf[-1] # normalize\n",
    "\n",
    "  # use linear interpolation of cdf to find new pixel values\n",
    "  image_equalized = np.interp(image.flatten(), bins[:-1], cdf)\n",
    "  #image_equalized = image_equalized.astype(int)\n",
    "\n",
    "  return image_equalized.reshape(image.shape), cdf\n",
    "\n",
    "\n",
    "def status_update(frame_number,tot_frames):\n",
    "  percent = frame_number * 100 // tot_frames\n",
    "\n",
    "  if percent % 10 == 0:\n",
    "      sys.stdout.write(\"%.0f%%\" %percent)\n",
    "      sys.stdout.flush()\n",
    "  else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "\n",
    "\n",
    "def check_duplicates(img_list, num_pixels, img_height, img_width):\n",
    "    \"\"\"this function utilizes a hash function to check for duplicates\n",
    "    that calculates a linear combination of a random assortment of \n",
    "    image pixels across all images in a directory.\"\"\"\n",
    "    \n",
    "    print(\"Checking for duplicates.\")\n",
    "    dup_table = {}\n",
    "    k=0\n",
    "    \n",
    "    rand_row = random.sample(xrange(img_height), num_pixels) #720 rows\n",
    "    rand_col = random.sample(xrange(img_width), num_pixels) #1280 cols\n",
    "    rand_pix = [(rand_row[i], rand_col[i]) for i in range(num_pixels)]\n",
    "    \n",
    "    for image in img_list:\n",
    "        img = Image.open(image).convert(\"L\") #read in\n",
    "        img = np.array(img, dtype=\"uint8\")  #convert to array\n",
    "        hash_val = 0\n",
    "        \n",
    "        for i in range(num_pixels):\n",
    "            pixels = rand_pix[i]\n",
    "            hash_val += (i+1)*img[pixels[0]][pixels[1]]\n",
    "            \n",
    "        if hash_val in dup_table:\n",
    "            dup_table[hash_val].append(image)\n",
    "        else:\n",
    "            dup_table[hash_val] = [image]\n",
    "        \n",
    "        k += 1\n",
    "        \n",
    "        # STATUS UPDATE\n",
    "        status_update(frame_number = k,tot_frames = len(img_list))\n",
    "            \n",
    "    for images in dup_table.values():\n",
    "        if len(images) > 1: \n",
    "            print(\"Potential Duplicates found:\", images)\n",
    "    \n",
    "    print()\n",
    "    print(\"Duplicate Check completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Duplicates\n",
    "\n",
    "The 'check_duplicates' function implements a hash function that generates a linear combination of randomly selected pixel values across all images in the image directory.  Hash values are stored to a dictionary as keys, with file names as values.  Potential duplicates are returned in a print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_duplicates(img_list = files, \n",
    "                 num_pixels = 100, \n",
    "                 img_height = frame_height, \n",
    "                 img_width = frame_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Histograms routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INITIALIZE INPUT DATA ARRAY FOR CLUSTERING OPERATION\n",
    "# INITIALIZE LABEL ARRAY\n",
    "X = np.empty((num_frames, (num_roi_tiles * num_bins)))\n",
    "y = np.empty(num_frames, dtype=int)\n",
    "k = 0\n",
    "start = time.time()\n",
    "print (\"Histogramming frames...\")\n",
    "\n",
    "\n",
    "# the following loops through all frames in a directory\n",
    "for keyframe in files:\n",
    "  # READ IN IMAGE TO NUMPY ARRAY AND CONVERT TO B&W\n",
    "  input_frame = keyframe  \n",
    "  img = Image.open(input_frame).convert(\"L\")\n",
    "  img = np.array(img, dtype=\"uint8\")\n",
    "    \n",
    "  # CROP IMAGE TO ROI\n",
    "  img = img[0:(tile_height)*5,:]\n",
    "\n",
    "  # \"EQUALIZE HISTOGRAM\" ACROSS ENTIRE ROI\n",
    "  img, _ = image_histogram_equalization(img, number_bins = 256)\n",
    "\n",
    "  # Initialize numpy array to hold global flattened histogram values for frame\n",
    "  flat_hist = np.zeros(num_roi_tiles * num_bins) # 96\n",
    "\n",
    "  # initialize some counters\n",
    "  tile_index = 0  \n",
    "  l = 0  \n",
    "\n",
    "  # the following loops through every tile in the ROI\n",
    "  for i in range(tiles_per_column): #for every row top to bottom  \n",
    "    for j in range(tiles_per_row): #for every column left to right\n",
    "      if tile_index in roi_tiles:\n",
    "        # crop a tile from the ROI\n",
    "        tile = img[i*tile_height:(i+1)*tile_height,j*tile_width:(j+1)*tile_width]\n",
    "        \n",
    "        # make a flattened bilevel histogram according to user-defined threshold\n",
    "        tile_histogram = np.histogram(tile, bins=[0, threshold, 256], range = (0,256), density = False) #auto flattens\n",
    "        \n",
    "        # normalize the histogram and add it to the global flattened histograms vector for the frame\n",
    "        flat_hist[l*num_bins:(l+1)*num_bins] = tile_histogram[0]/tile_histogram[0].sum()\n",
    "        \n",
    "        # increment counters\n",
    "        tile_index += 1\n",
    "        l += 1\n",
    "      else:\n",
    "        tile_index += 1\n",
    "\n",
    "  # add flattened global histogram to data\n",
    "  X[k] = flat_hist \n",
    "\n",
    "  # STATUS UPDATE\n",
    "  status_update(frame_number = k,tot_frames = num_frames)\n",
    "    \n",
    "  k += 1\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print ()\n",
    "print (\"Histogram-ization for all frames: %.2f seconds\" %(end - start))\n",
    "print(\"Histogram-ization completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INITIALIZE CLUSTERING FROM SKLEARN LIBRARY\n",
    "kmeans = KMeans(n_clusters = num_scenes, \n",
    "                init='k-means++', \n",
    "                n_init=10, \n",
    "                max_iter=300, \n",
    "                tol=0.0001).fit(X,y)\n",
    "\n",
    "# STORE PREDECTIONS TO ARRAY\n",
    "y_hat = kmeans.labels_\n",
    "\n",
    "# STORE DISTANCES TO ARRAY\n",
    "distances = KMeans.transform(kmeans,X)\n",
    "\n",
    "# PRINT OUTPUT\n",
    "print(\"Clustering completed.  Scene counts: \",np.bincount(y_hat))\n",
    "print(\"Distance to cluster centers available as \\\"distances\\\".\")\n",
    "print(\"Predicted labels available as \\\"y_hat\\\".\")\n",
    "print(\"Cluster score on frame directory: \",kmeans.score(X))\n",
    "#print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING CLUSTERS\n",
    "\n",
    "Analysis to explore the clustering behavior of the camera feed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA-reduced 2D plots of Cluster Op\n",
    "\n",
    "The following reduces multi-variate histograms into two-dimensional vectors for easy visualization of clustering behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reduced_data = PCA(n_components=2).fit_transform(X)\n",
    "kmeans = KMeans(init='k-means++',\n",
    "                n_clusters = num_scenes,\n",
    "                n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the camera frames (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification of frames nearest the decision boundaries\n",
    "\n",
    "The following uses an implementation of the closest-pair problem to identify frames that are closest to the decision boundaries.  From an intuitive standpoint, these frames are the \"most-difficult\" to categorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NUMBER OF FRAMES TO IDENTIFY\n",
    "num_difficult_frames = 10\n",
    "\n",
    "# SORT\n",
    "sort_dist = np.sort(distances, axis=-1, kind='quicksort', order=None)\n",
    "\n",
    "# CALCULATE DISTANCES TO CLUSTER CENTERS\n",
    "cluster_dist = np.empty((num_frames, num_scenes - 1 ))\n",
    "min_dist = np.empty ((num_frames, 1))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    for j in range(num_scenes - 1) :\n",
    "        cluster_dist[i,j] = sort_dist[i,j+1]-sort_dist[i,j]\n",
    "    min_dist[i] = min(cluster_dist[i])\n",
    "\n",
    "# RETURN INDEX OF MOST-DIFFICULT FRAMES\n",
    "difficult_frames = sorted(range(len(min_dist)), key=lambda k: min_dist[k])[0:num_difficult_frames] #narrowest to largest distance\n",
    "print(\"Difficult frames: \", difficult_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VIEW DIFFICULT FRAMES\n",
    "most_difficult = difficult_frames[0]\n",
    "img = Image.open(files[most_difficult])\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
